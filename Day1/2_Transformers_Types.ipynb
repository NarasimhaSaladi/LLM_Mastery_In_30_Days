{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main types of transformers. They are:\n",
    "- Encoder Only\n",
    "- Decoder Only\n",
    "- Encoder Decoder\n",
    "![types](res/6_transformers_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-only Transformers - BERT Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**: \n",
    "Encoder-only Transformers consist of a stack of encoder layers. Each layer has two main components:\n",
    "1. Multi-head self-attention mechanism\n",
    "2. Feedforward neural network\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Bidirectional context: Can look at the entire input sequence in both directions.\n",
    "- Suitable for tasks that require understanding of the entire input context.\n",
    "\n",
    "**Examples**: \n",
    "BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, ALBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Cases**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Text Classification**\n",
    "   - Sentiment Analysis\n",
    "   - Topic Classification\n",
    "   - Spam Detection\n",
    "\n",
    "   Example (Sentiment Analysis with BERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Vasanth\\anaconda\\envs\\omnipostai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Vasanth\\anaconda\\envs\\omnipostai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"I absolutely loved this movie! The acting was superb.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "outputs = model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "print(f\"Sentiment: {'Positive' if prediction == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** BERT is particularly effective for sentiment analysis because it can understand context and nuance in language. Its bidirectional nature allows it to capture relationships between words in both directions, which is crucial for understanding sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Named Entity Recognition (NER)**\n",
    "   - Identifying and classifying named entities (e.g., person names, organizations, locations) in text.\n",
    "\n",
    "   Example (NER with BERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Vasanth\\anaconda\\envs\\omnipostai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9996038, 'index': 1, 'word': 'Apple', 'start': 0, 'end': 5}, {'entity': 'I-ORG', 'score': 0.9994128, 'index': 2, 'word': 'Inc', 'start': 6, 'end': 9}, {'entity': 'B-LOC', 'score': 0.99944466, 'index': 12, 'word': 'New', 'start': 46, 'end': 49}, {'entity': 'I-LOC', 'score': 0.9993468, 'index': 13, 'word': 'York', 'start': 50, 'end': 54}, {'entity': 'I-LOC', 'score': 0.9995746, 'index': 14, 'word': 'City', 'start': 55, 'end': 59}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Apple Inc. is planning to open a new store in New York City.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** Encoder-only Transformers excel at NER tasks because they can understand the context around each word, which is crucial for accurately identifying and classifying named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Question Answering**\n",
    "   - Extracting answers from a given context based on questions.\n",
    "\n",
    "   Example (Question Answering with BERT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 1.1810, -0.4073,  0.9986,  ..., -0.7445,  0.0380, -0.5510],\n",
      "         [ 1.6172, -0.6785,  1.6932,  ..., -0.8216, -0.2387, -0.6187],\n",
      "         [ 2.0840, -0.5496,  1.3313,  ..., -0.7791,  0.1698, -0.3950],\n",
      "         ...,\n",
      "         [ 0.2879, -0.1813,  1.2631,  ..., -0.2022,  0.4699,  0.5535],\n",
      "         [ 0.6069, -0.1943,  0.7584,  ..., -0.5106, -0.4027, -0.4910],\n",
      "         [ 1.0183, -0.8215,  0.9088,  ..., -0.8094,  0.8372, -0.2027]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** The bidirectional nature of encoder-only Transformers allows them to understand the relationship between the question and the context, making them highly effective for question-answering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder-only Transformers - GPT family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**: \n",
    "Decoder-only Transformers consist of a stack of decoder layers. Each layer typically includes:\n",
    "1. Masked multi-head self-attention mechanism\n",
    "2. Feedforward neural network\n",
    "3. Final Head/ Softmax layer - Projection\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Autoregressive: Generates output sequentially, one element at a time.\n",
    "- Unidirectional context: Each position can only attend to previous positions.\n",
    "\n",
    "**Examples**: \n",
    "GPT (Generative Pre-trained Transformer) series, CTRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Usecases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Generation / Text Completion**\n",
    "   - Creative Writing\n",
    "   - Code Completion\n",
    "   - Chatbots\n",
    "\n",
    "   Example (Text Generation with GPT-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where AI has become sentient, it's hard to imagine a better time to be a human.\n",
      "\n",
      "\"We're going to have to start thinking about how we're doing things,\" said Dr. Michael S. Hirsch, a professor of psychology at the University of California, San Francisco. \"We need to think about what we can do to make it better.\"\n",
      "...\n",
      ", which is a collection of essays by the author, including a series of short stories by\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "prompt = \"In a world where AI has become sentient,\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** Decoder-only Transformers are ideal for text generation because they can generate coherent and contextually relevant text by predicting the next token based on the previous ones. This autoregressive nature allows them to maintain consistency and coherence in longer generated sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Transformers - T5 Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**: \n",
    "Encoder-Decoder Transformers, also known as sequence-to-sequence (seq2seq) Transformers, consist of both encoder and decoder stacks. The architecture includes:\n",
    "1. Encoder stack (similar to encoder-only Transformers)\n",
    "2. Decoder stack (similar to decoder-only Transformers)\n",
    "3. Cross-attention mechanism connecting encoder and decoder\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Suitable for tasks that transform one sequence into another.\n",
    "- Can handle input and output sequences of different lengths.\n",
    "- Combines benefits of both encoder and decoder architectures.\n",
    "\n",
    "**Examples**: \n",
    "T5 (Text-to-Text Transfer Transformer), BART, Marian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Machine Translation:**\n",
    "    - Translating text from one language to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Vasanth\\anaconda\\envs\\omnipostai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Vasanth\\anaconda\\envs\\omnipostai\\lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "input_text = \"translate English to German: The house is wonderful.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** Encoder-Decoder Transformers are ideal for machine translation because they can encode the meaning of the input sequence in one language and decode it into another language. The encoder captures the context of the input, while the decoder generates the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Text Summarization**\n",
    "   - Generating concise summaries of longer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI research has been defined as the field of study of intelligent agents. the term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills associated with the human mind.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals. The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"summarize: \" + long_text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:** Encoder-Decoder Transformers are well-suited for summarization tasks because they can encode the important information from a long input text and decode it into a concise summary. The encoder captures the key points, while the decoder generates a coherent summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have revolutionized the field of NLP and continue to find applications in various domains. Each type of Transformer - encoder-only, decoder-only, and encoder-decoder - has its strengths and is suited for different types of tasks. \n",
    "\n",
    "- **Encoder-only Transformers** excel at understanding and analyzing text, making them ideal for classification, named entity recognition, and question answering tasks. \n",
    "- **Decoder-only Transformers** are powerful for text generation tasks, including creative writing, code completion, and language modeling. \n",
    "- **Encoder-Decoder Transformers** combine the strengths of both architectures, making them suitable for tasks that involve transforming one sequence into another, such as translation, summarization, question answering and certain types of conversational AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipostai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
