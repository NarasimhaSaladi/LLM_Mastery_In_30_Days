{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERMEDIATE PREREQUISITES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Models: RNN, LSTM, Bi-LSTM, and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Introduction\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to work with sequential data. Unlike feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain an internal state or \"memory\" of previous inputs.\n",
    "\n",
    "### How and Why the Model was Created\n",
    "RNNs were developed in the 1980s to address the limitation of traditional neural networks in processing sequential data. The key motivation was to create a network that could use its internal memory to process sequences of inputs, making them suitable for tasks like speech recognition and language modeling.\n",
    "\n",
    "### Detailed Working Explanation\n",
    "1. **Basic Structure**: An RNN consists of input, hidden, and output layers. The hidden layer has a self-loop connection, allowing it to pass information from one time step to the next.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   At each time step t, the RNN takes an input x_t and the previous hidden state h_(t-1) to compute the current hidden state h_t:\n",
    "   h_t = tanh(W_hx * x_t + W_hh * h_(t-1) + b_h)\n",
    "   Where W_hx, W_hh are weight matrices, and b_h is a bias vector.\n",
    "\n",
    "3. **Output Computation**:\n",
    "   The output y_t is computed based on the current hidden state:\n",
    "   y_t = W_hy * h_t + b_y\n",
    "   Where W_hy is a weight matrix and b_y is a bias vector.\n",
    "\n",
    "4. **Backpropagation Through Time (BPTT)**:\n",
    "   RNNs are trained using BPTT, which unrolls the network through time and applies backpropagation. The gradients are computed for each time step and summed.\n",
    "\n",
    "5. **Gradient Flow**:\n",
    "   During training, gradients can vanish or explode as they're propagated back through time, making it difficult to learn long-term dependencies.\n",
    "\n",
    "6. **Example**: In a character-level language model, each input x_t is a character, and the network predicts the next character y_t. The hidden state h_t captures the context of previous characters.\n",
    "\n",
    "### When to Use (Use Cases)\n",
    "- Time series prediction\n",
    "- Natural language processing tasks (e.g., sentiment analysis, text generation)\n",
    "- Speech recognition\n",
    "- Music generation\n",
    "\n",
    "### Advantages\n",
    "- Can process sequences of variable length\n",
    "- Shares parameters across time steps, reducing the number of parameters to learn\n",
    "- Capable of capturing temporal dependencies in data\n",
    "\n",
    "### Disadvantages\n",
    "- Suffers from vanishing and exploding gradient problems\n",
    "- Difficulty in capturing long-term dependencies\n",
    "- Can be computationally expensive for very long sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Introduction\n",
    "Long Short-Term Memory (LSTM) networks are a specialized form of RNNs designed to capture long-term dependencies in sequential data. They use a gating mechanism to control the flow of information, allowing them to remember or forget information selectively.\n",
    "\n",
    "### How and Why the Model was Created\n",
    "LSTMs were introduced to address the vanishing gradient problem faced by traditional RNNs. The goal was to create a model that could learn and remember information over long sequences, which is crucial for many real-world applications.\n",
    "\n",
    "### Detailed Working Explanation\n",
    "1. **LSTM Cell Structure**: An LSTM cell consists of a cell state and three gates: forget gate, input gate, and output gate.\n",
    "\n",
    "2. **Forget Gate**:\n",
    "   f_t = σ(W_f * [h_(t-1), x_t] + b_f)\n",
    "   This gate decides what information to discard from the cell state.\n",
    "\n",
    "3. **Input Gate**:\n",
    "   i_t = σ(W_i * [h_(t-1), x_t] + b_i)\n",
    "   c̃_t = tanh(W_c * [h_(t-1), x_t] + b_c)\n",
    "   This gate decides what new information to store in the cell state.\n",
    "\n",
    "4. **Cell State Update**:\n",
    "   c_t = f_t * c_(t-1) + i_t * c̃_t\n",
    "   The cell state is updated based on the forget and input gates.\n",
    "\n",
    "5. **Output Gate**:\n",
    "   o_t = σ(W_o * [h_(t-1), x_t] + b_o)\n",
    "   h_t = o_t * tanh(c_t)\n",
    "   This gate controls what information from the cell state is output.\n",
    "\n",
    "6. **Example**: In a sentiment analysis task, the LSTM can learn to focus on key words or phrases that strongly indicate sentiment, while forgetting less relevant information.\n",
    "\n",
    "### When to Use (Use Cases)\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Sentiment analysis\n",
    "- Time series forecasting with long-term trends\n",
    "\n",
    "### Advantages\n",
    "- Capable of learning long-term dependencies\n",
    "- Mitigates the vanishing gradient problem\n",
    "- Selective memory through gating mechanism\n",
    "- Robust performance across a wide range of sequence lengths\n",
    "\n",
    "### Disadvantages\n",
    "- More complex than standard RNNs, requiring more computational resources\n",
    "- Can be challenging to train, requiring careful initialization and hyperparameter tuning\n",
    "- Potential for overfitting, especially on smaller datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/bilistm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Introduction\n",
    "Bidirectional LSTM (Bi-LSTM) is an extension of the standard LSTM that processes input sequences in both forward and backward directions. This allows the network to capture context from both past and future states, providing a more comprehensive understanding of the sequence.\n",
    "\n",
    "### How and Why the Model was Created\n",
    "Bi-LSTMs were developed to address the limitation of unidirectional LSTMs in tasks where future context is as important as past context. They were introduced to improve performance in tasks like speech recognition and natural language processing, where understanding the full context of a sequence is crucial.\n",
    "\n",
    "### Detailed Working Explanation\n",
    "1. **Network Structure**: A Bi-LSTM consists of two separate LSTM layers: one processes the input sequence from left to right (forward), and the other from right to left (backward).\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   Forward LSTM: h_f_t = LSTM_f(x_t, h_f_(t-1))\n",
    "   Backward LSTM: h_b_t = LSTM_b(x_t, h_b_(t+1))\n",
    "   Where LSTM_f and LSTM_b are the forward and backward LSTM functions respectively.\n",
    "\n",
    "3. **Output Combination**:\n",
    "   The outputs from both directions are combined, often by concatenation:\n",
    "   h_t = [h_f_t, h_b_t]\n",
    "\n",
    "4. **Final Output**:\n",
    "   y_t = W_y * h_t + b_y\n",
    "   Where W_y is a weight matrix and b_y is a bias vector.\n",
    "\n",
    "5. **Training**: Both forward and backward passes are trained simultaneously using backpropagation through time.\n",
    "\n",
    "6. **Example**: In named entity recognition, a Bi-LSTM can use both preceding and following words to accurately classify an entity, which is particularly useful for disambiguating entities based on context.\n",
    "\n",
    "### When to Use (Use Cases)\n",
    "- Named Entity Recognition\n",
    "- Part-of-speech tagging\n",
    "- Machine translation\n",
    "- Sentiment analysis where whole-sentence context is important\n",
    "\n",
    "### Advantages\n",
    "- Captures both past and future context\n",
    "- Improves performance in tasks where bidirectional context is crucial\n",
    "- Reduces ambiguity in classification tasks\n",
    "- Can be combined with attention mechanisms for even better performance\n",
    "\n",
    "### Disadvantages\n",
    "- Increased computational complexity compared to unidirectional LSTMs\n",
    "- Requires the entire sequence to be available before processing, making it unsuitable for real-time applications\n",
    "- Potential for overfitting, especially on smaller datasets\n",
    "- More complex to implement and tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/gru.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Introduction\n",
    "Gated Recurrent Units (GRUs) are a type of recurrent neural network designed as a simpler alternative to LSTMs. They use a gating mechanism to control information flow but with a simplified structure compared to LSTMs.\n",
    "\n",
    "### How and Why the Model was Created\n",
    "GRUs were introduced as part of an effort to create a more computationally efficient alternative to LSTMs. The goal was to maintain the ability to capture long-term dependencies while reducing the number of parameters and computational complexity.\n",
    "\n",
    "### Detailed Working Explanation\n",
    "1. **GRU Cell Structure**: A GRU cell consists of two gates: reset gate and update gate.\n",
    "\n",
    "2. **Update Gate**:\n",
    "   z_t = σ(W_z * [h_(t-1), x_t])\n",
    "   This gate decides how much of the past information to pass along to the future.\n",
    "\n",
    "3. **Reset Gate**:\n",
    "   r_t = σ(W_r * [h_(t-1), x_t])\n",
    "   This gate decides how much of the past information to forget.\n",
    "\n",
    "4. **Candidate Hidden State**:\n",
    "   h̃_t = tanh(W * [r_t * h_(t-1), x_t])\n",
    "   This is the new memory content, which will be used to update the hidden state.\n",
    "\n",
    "5. **Hidden State Update**:\n",
    "   h_t = (1 - z_t) * h_(t-1) + z_t * h̃_t\n",
    "   The hidden state is updated based on the update gate and the candidate hidden state.\n",
    "\n",
    "6. **Example**: In a text classification task, the GRU can learn to focus on key phrases while ignoring less relevant parts of the input sequence.\n",
    "\n",
    "### When to Use (Use Cases)\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Time series prediction with moderate sequence lengths\n",
    "- When computational efficiency is a priority\n",
    "\n",
    "### Advantages\n",
    "- Simpler architecture than LSTMs, with fewer parameters\n",
    "- Generally faster to train and run than LSTMs\n",
    "- Effective at capturing medium to long-range dependencies\n",
    "- Often performs comparably to LSTMs on many tasks\n",
    "\n",
    "### Disadvantages\n",
    "- May be less powerful than LSTMs for some complex tasks requiring fine-grained control over memory\n",
    "- Less studied and understood compared to LSTMs\n",
    "- Performance can vary depending on the specific task and dataset\n",
    "- May struggle with very long-term dependencies compared to LSTMs in some cases"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
