{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERMEDIATE PREREQUISITES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning: What, Why, When, How\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What is Deep Learning?**\n",
    "\n",
    "Deep Learning is a subset of Machine Learning that uses artificial neural networks with multiple layers (deep neural networks) to model and solve complex problems. It's inspired by the structure and function of the human brain.\n",
    "\n",
    "#### **Why Deep Learning?**\n",
    "\n",
    "1. **Automatic Feature Extraction**: Deep learning models can automatically learn features from raw data, reducing the need for manual feature engineering.\n",
    "2. **Scalability**: Performance often improves with more data and larger models.\n",
    "3. **Versatility**: Applicable to a wide range of problems, from image and speech recognition to natural language processing and game playing.\n",
    "4. **State-of-the-art Performance**: Achieves top results in many domains, often surpassing human-level performance.\n",
    "\n",
    "#### **When to Use Deep Learning?**\n",
    "\n",
    "- **Large Amounts of Data**: Deep learning thrives on big data.\n",
    "- **Complex Problems**: When traditional ML methods struggle with high-dimensional or highly nonlinear problems.\n",
    "- **Unstructured Data**: Especially effective for images, audio, and text.\n",
    "- **Time Series and Sequential Data**: RNNs and LSTMs excel at these tasks.\n",
    "- **Autonomous Systems**: In robotics, self-driving cars, etc.\n",
    "\n",
    "#### **How Deep Learning Works?**\n",
    "\n",
    "1. **Data Preparation**: Collect and preprocess large datasets.\n",
    "2. **Model Architecture Design**: Choose and configure a suitable neural network architecture.\n",
    "3. **Training**: \n",
    "   - Feed data through the network (forward propagation)\n",
    "   - Calculate loss\n",
    "   - Update weights (backpropagation)\n",
    "4. **Evaluation**: Test the model on unseen data.\n",
    "5. **Deployment**: Use the trained model for predictions on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Deep Learning over Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1. **Feature Learning**: \n",
    "   - ML: Often requires manual feature engineering\n",
    "   - DL: Automatically learns relevant features\n",
    "\n",
    "2. **Performance with Large Data**: \n",
    "   - ML: Performance plateaus with increasing data\n",
    "   - DL: Continues to improve with more data\n",
    "\n",
    "3. **Handling Unstructured Data**: \n",
    "   - ML: Struggles with raw unstructured data\n",
    "   - DL: Excels at processing raw images, audio, and text\n",
    "\n",
    "4. **Scalability**: \n",
    "   - ML: Often requires redesigning as problem complexity increases\n",
    "   - DL: Can scale to very complex problems by adding layers/neurons\n",
    "\n",
    "5. **Transfer Learning**: \n",
    "   - ML: Limited transfer learning capabilities\n",
    "   - DL: Pretrained models can be fine-tuned for new tasks efficiently\n",
    "\n",
    "6. **Parallel Processing**: \n",
    "   - ML: Limited parallelization options\n",
    "   - DL: Highly parallelizable, leveraging GPUs for faster computation\n",
    "\n",
    "However, deep learning also has disadvantages, including:\n",
    "- Requires large amounts of data\n",
    "- Computationally intensive\n",
    "- Less interpretable (\"black box\" nature)\n",
    "- Prone to overfitting without proper regularization\n",
    "\n",
    "The choice between ML and DL depends on the specific problem, available data, and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It provides a flexible and efficient platform for building and training neural networks, making it a popular choice among researchers and developers in the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Dynamic Computational Graphs**: PyTorch uses a dynamic computation graph, allowing for more flexible model architectures and easier debugging.\n",
    "\n",
    "2. **Pythonic Interface**: PyTorch provides a natural, intuitive interface that aligns well with Python programming practices.\n",
    "\n",
    "3. **GPU Acceleration**: Built-in support for CUDA enables seamless utilization of GPU resources for faster computations.\n",
    "\n",
    "4. **Autograd System**: Automatic differentiation engine that enables automatic computation of gradients, simplifying the implementation of backpropagation.\n",
    "\n",
    "5. **Rich Ecosystem**: Extensive libraries and tools for various deep learning tasks, including computer vision, natural language processing, and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Functional Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's functionality can be broadly categorized into several key areas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tensor Operations\n",
    "\n",
    "At the core of PyTorch are tensors, multi-dimensional arrays similar to NumPy's ndarrays but with the ability to run on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[1.9458, 2.7273, 3.1516],\n",
      "        [1.6086, 2.0129, 3.8765],\n",
      "        [1.9567, 2.9389, 3.7240]])\n",
      "tensor([[1.4821, 0.8395, 0.8906],\n",
      "        [1.4219, 1.2658, 0.7381],\n",
      "        [2.1689, 1.3877, 1.4922]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating tensors\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.rand(3, 3)\n",
    "# [[1,2,3], [4,5,6], [7,8,9]]\n",
    "print(y.shape)\n",
    "\n",
    "# Basic operations\n",
    "z = x + y\n",
    "print(z)\n",
    "w = torch.matmul(y, y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9458, 0.7273, 0.1516],\n",
      "        [0.6086, 0.0129, 0.8765],\n",
      "        [0.9567, 0.9389, 0.7240]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Autograd (Automatic Differentiation)\n",
    "\n",
    "PyTorch's autograd system enables automatic computation of gradients, which is crucial for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()  # Computes gradients\n",
    "print(x.grad)  # Displays gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network Modules\n",
    "\n",
    "PyTorch provides a high-level API for building neural networks through the `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Loading and Processing\n",
    "\n",
    "The `torch.utils.data` module provides tools for efficient data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "data, labels = [\"a\", \"b\"], [1, 2]\n",
    "\n",
    "dataset = CustomDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These components work together to provide a comprehensive framework for developing, training, and deploying deep learning models. PyTorch's design philosophy emphasizes ease of use, flexibility, and performance, making it a powerful tool for both research and production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the `Dataset` class is an abstract class representing a dataset. Custom datasets should inherit from `Dataset` and override the following methods:\n",
    "\n",
    "- `__len__`: Returns the size of the dataset\n",
    "- `__getitem__`: Supports integer indexing from 0 to len(self)\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "customdata = CustomDataset([\"a\", \"b\"], [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `DataLoader` class provides an iterator over the dataset and supports:\n",
    "\n",
    "- Automatic batching\n",
    "- Shuffling\n",
    "- Multiprocessing for data loading\n",
    "\n",
    "Key parameters:\n",
    "- `batch_size`: Number of samples in each batch\n",
    "- `shuffle`: Whether to shuffle the data at every epoch\n",
    "- `num_workers`: Number of subprocesses for data loading\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a',)\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CustomDataset([\"a\", \"b\"], [1, 2])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    print(batch_data)\n",
    "    print(batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Neural Networks?\n",
    "\n",
    "Neural Networks are computational models inspired by the human brain's structure and function. They consist of interconnected nodes (neurons) organized in layers, designed to recognize patterns and solve complex problems.\n",
    "\n",
    "### Why use Neural Networks?\n",
    "\n",
    "1. **Pattern Recognition**: Excellent at identifying complex patterns in data.\n",
    "2. **Adaptability**: Can learn and improve from experience.\n",
    "3. **Generalization**: Can make accurate predictions on unseen data.\n",
    "4. **Non-linearity**: Can model complex non-linear relationships.\n",
    "5. **Parallel Processing**: Can process multiple inputs simultaneously.\n",
    "\n",
    "### How do Neural Networks work?\n",
    "\n",
    "1. **Input Layer**: Receives initial data.\n",
    "2. **Hidden Layers**: Process the data through weighted connections.\n",
    "3. **Output Layer**: Produces the final result.\n",
    "4. **Activation Functions**: Introduce non-linearity to the model.\n",
    "5. **Training**: Adjust weights to minimize the difference between predicted and actual outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Perceptron from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is the simplest form of a neural network, consisting of a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Prediction: 0\n",
      "Input: [0 1], Prediction: 0\n",
      "Input: [1 0], Prediction: 0\n",
      "Input: [1 1], Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activate(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activate(sum)\n",
    "\n",
    "    def train(self, inputs, label):\n",
    "        prediction = self.predict(inputs)\n",
    "        error = label - prediction\n",
    "        self.weights += error * self.learning_rate * inputs\n",
    "        self.bias += error * self.learning_rate\n",
    "\n",
    "# Example usage\n",
    "p = Perceptron(2)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "for _ in range(100):\n",
    "    for inputs, label in zip(X, y):\n",
    "        p.train(inputs, label)\n",
    "\n",
    "# Test\n",
    "for inputs in X:\n",
    "    print(f\"Input: {inputs}, Prediction: {p.predict(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple perceptron can learn to perform basic logical operations like AND or OR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNs are the most basic type of neural network, consisting of fully connected layers.\n",
    "\n",
    "- **Structure**: Input layer, one or more hidden layers, output layer.\n",
    "- **Use Cases**: Classification, regression, pattern recognition.\n",
    "- **Pros**: Versatile, can approximate any function.\n",
    "- **Cons**: May struggle with spatial or temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are specialized for processing grid-like data, such as images.\n",
    "\n",
    "- **Key Components**: Convolutional layers, pooling layers, fully connected layers.\n",
    "- **Use Cases**: Image classification, object detection, computer vision tasks.\n",
    "- **Pros**: Efficient for spatial data, parameter sharing reduces overfitting.\n",
    "- **Cons**: May struggle with non-spatial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are designed to work with sequential data by maintaining an internal state (memory).\n",
    "\n",
    "- **Key Feature**: Loops in the network allow information to persist.\n",
    "- **Variants**: LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit).\n",
    "- **Use Cases**: Natural language processing, time series prediction, speech recognition.\n",
    "- **Pros**: Can handle variable-length sequences, maintains temporal information.\n",
    "- **Cons**: Can be difficult to train due to vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ANN Network - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A multilayer ANN, also known as a feedforward neural network, consists of multiple layers of neurons.\n",
    "\n",
    "1. **Input Layer**: Receives the initial data.\n",
    "2. **Hidden Layers**: Process the data through weighted connections.\n",
    "3. **Output Layer**: Produces the final result.\n",
    "4. **Activation Functions**: Introduce non-linearity (e.g., ReLU, sigmoid, tanh).\n",
    "5. **Backpropagation**: Algorithm used to train the network by adjusting weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Simple ANN Network - Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a high-level API for creating neural networks through the `torch.nn` module.\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "1. Define a class that inherits from `nn.Module`\n",
    "2. Define layers in the `__init__` method\n",
    "3. Implement the `forward` method to define the computation\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "model = SimpleNN(input_size=10, hidden_size=20, output_size=2)\n",
    "```\n",
    "\n",
    "### Common Layer Types\n",
    "\n",
    "- `nn.Linear`: Fully connected layer\n",
    "- `nn.Conv2d`: 2D convolutional layer\n",
    "- `nn.RNN`, `nn.LSTM`, `nn.GRU`: Recurrent layers\n",
    "- `nn.BatchNorm2d`: Batch normalization\n",
    "- `nn.Dropout`: Dropout layer\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "- `nn.ReLU`: Rectified Linear Unit\n",
    "- `nn.Sigmoid`: Sigmoid activation\n",
    "- `nn.Tanh`: Hyperbolic tangent\n",
    "- `nn.Softmax`: Softmax activation\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "- `nn.MSELoss`: Mean Squared Error\n",
    "- `nn.CrossEntropyLoss`: Combines LogSoftmax and NLLLoss\n",
    "- `nn.BCELoss`: Binary Cross Entropy\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Available in `torch.optim`:\n",
    "\n",
    "- `optim.SGD`: Stochastic Gradient Descent\n",
    "- `optim.Adam`: Adaptive Moment Estimation\n",
    "- `optim.RMSprop`: Root Mean Square Propagation\n",
    "\n",
    "#### Training Loop Example:\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "This structure allows for flexible and efficient creation of various neural network architectures in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Simple ANN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6866\n",
      "Epoch [20/100], Loss: 0.6865\n",
      "Epoch [30/100], Loss: 0.6864\n",
      "Epoch [40/100], Loss: 0.6894\n",
      "Epoch [50/100], Loss: 0.6859\n",
      "Epoch [60/100], Loss: 0.6827\n",
      "Epoch [70/100], Loss: 0.6827\n",
      "Epoch [80/100], Loss: 0.6846\n",
      "Epoch [90/100], Loss: 0.6826\n",
      "Epoch [100/100], Loss: 0.6834\n",
      "Test Accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset class\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Simple feed-forward neural network\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(samples=1000, input_size=2, num_classes=2):\n",
    "    X = np.random.randn(samples, input_size)\n",
    "    y = np.random.randint(0, num_classes, samples)\n",
    "    return X, y\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 2  # Number of features in the input data\n",
    "hidden_size = 16  # Hidden layer size\n",
    "output_size = 2  # Number of classes (binary classification)\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32  # Mini-batch size\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data(samples=1000, input_size=input_size, num_classes=output_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = RandomDataset(X_train, y_train)\n",
    "test_dataset = RandomDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleANN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()  # Since it's a classification problem\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Inference function\n",
    "def inference(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Testing the model on test data\n",
    "test_accuracy = inference(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What: The process of computing the output of a neural network given an input.\n",
    "- Why: To make predictions and compute the loss.\n",
    "- How: By sequentially applying each layer's operations to the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- What: The process of computing gradients of the loss with respect to the network parameters.\n",
    "- Why: To determine how to adjust the parameters to minimize the loss.\n",
    "- How: By applying the chain rule of calculus to propagate gradients backwards through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions - What, Why, How, Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Activation Functions?\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. They are applied to the weighted sum of the inputs at each neuron.\n",
    "\n",
    "### Why use Activation Functions?\n",
    "\n",
    "1. Introduce non-linearity: This allows the network to learn complex patterns.\n",
    "2. Normalize the output: Keep values within a specific range.\n",
    "3. Enable backpropagation: Many activation functions are differentiable, allowing gradients to flow backward through the network.\n",
    "\n",
    "### How do Activation Functions work?\n",
    "\n",
    "They take the weighted sum of inputs to a neuron and apply a mathematical operation to produce an output. This output then becomes the input for the next layer or the final output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Sigmoid (Logistic) Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: A S-shaped curve that maps any input value to a value between 0 and 1.\n",
    "\n",
    "**Why**: \n",
    "- Useful for models where we need to predict the probability as an output.\n",
    "- Historically popular, but less used in hidden layers of modern networks.\n",
    "\n",
    "**How**: \n",
    "\n",
    "```math\n",
    "\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "```\n",
    "\n",
    "![image](res/sigmoid.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "activation_input = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(activation_input.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.7311])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Hyperbolic Tangent (tanh)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: Similar to sigmoid, but maps values to range [-1, 1].\n",
    "\n",
    "**Why**: \n",
    "- Zero-centered, making it easier for the model to learn.\n",
    "- Often performs better than sigmoid in hidden layers.\n",
    "\n",
    "**How**:\n",
    "\n",
    "```math\n",
    "\n",
    "f ( x ) = exp(x) − exp(−x) / exp(x) + exp(−x)\n",
    "```\n",
    "\n",
    "![image](res/tanh.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = nn.Tanh()\n",
    "output = tanh(activation_input.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.7616])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Rectified Linear Unit (ReLU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: Returns 0 for negative values, and the input value for positive values.\n",
    "\n",
    "**Why**: \n",
    "- Computationally efficient.\n",
    "- Helps mitigate the vanishing gradient problem.\n",
    "- Induces sparsity in the hidden units.\n",
    "\n",
    "**How**: \n",
    "```math\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "![image](res/relu.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "output = relu(activation_input.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Leaky ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: Similar to ReLU, but allows small negative values when the input is less than zero.\n",
    "\n",
    "**Why**: \n",
    "- Attempts to solve the \"dying ReLU\" problem where neurons can get stuck during training.\n",
    "- Allows for slight gradient flow for negative inputs.\n",
    "\n",
    "**How**: \n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x, & \\text{if } x > 0 \\\\\n",
    "\\alpha x, & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $\\alpha$ is a small constant (e.g., $\\alpha = 0.01$).\n",
    "\n",
    "\n",
    "![image](res/leaky_relu.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = nn.LeakyReLU(0.01)  # 0.01 is the default negative slope\n",
    "output = leaky_relu(activation_input.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(torch.tensor(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(torch.tensor(-1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Softmax**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: Converts a vector of real numbers into a probability distribution.\n",
    "\n",
    "**Why**: \n",
    "- Commonly used in the output layer of multi-class classification problems.\n",
    "- Ensures all output values are between 0 and 1 and sum to 1.\n",
    "\n",
    "**How**: \n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)  # Apply softmax along dimension 1\n",
    "output = softmax(activation_input.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.7311])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivates of Activation Function\n",
    "\n",
    "![image1](res/activationfunccheatsheet.webp)\n",
    "![image2](res/activationfunctionderivates.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- For hidden layers, ReLU is often a good default choice due to its simplicity and effectiveness.\n",
    "- For binary classification output, sigmoid is commonly used.\n",
    "- For multi-class classification output, softmax is typically used.\n",
    "- For regression problems, the output layer often uses a linear activation (i.e., no activation function).\n",
    "\n",
    "The choice of activation function can significantly impact the network's performance and training dynamics. Experimentation is often necessary to find the best activation functions for a specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions - What, Why, How, Types and Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Loss Functions?\n",
    "Loss functions measure the difference between the predicted output and the actual target, quantifying the model's performance.\n",
    "\n",
    "### Why use Loss Functions?\n",
    "They guide the learning process by providing a scalar value to be minimized during training.\n",
    "\n",
    "### How do Loss Functions work?\n",
    "They compute a score based on the model's predictions and the true values, which is then used to update the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss Functions Types**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Mean Squared Error (MSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: A loss function that measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "**Why**: \n",
    "- Suitable for regression problems\n",
    "- Penalizes larger errors more heavily\n",
    "- Differentiable, making it suitable for gradient-based optimization\n",
    "\n",
    "**How**: \n",
    "- Calculate the difference between each predicted and actual value\n",
    "- Square these differences\n",
    "- Take the mean of these squared differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "predictions = torch.Tensor([1.0, 0.0, 1.0])\n",
    "targets = torch.Tensor([0.0, 0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Cross-Entropy Loss**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: A loss function that measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "**Why**: \n",
    "- Suitable for multi-class classification problems\n",
    "- Encourages confident predictions\n",
    "- Works well with softmax activation in the output layer\n",
    "\n",
    "**How**: \n",
    "- Apply softmax to the model's raw output to get probabilities\n",
    "- Take the negative log of the predicted probability of the correct class\n",
    "- Average this across all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(predictions, targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5514)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Binary Cross-Entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**What**: A special case of cross-entropy for binary classification problems.\n",
    "\n",
    "**Why**: \n",
    "- Suitable for binary classification problems\n",
    "- Works well with sigmoid activation in the output layer\n",
    "\n",
    "**How**: \n",
    "- Apply sigmoid to the model's raw output to get a probability\n",
    "- Compute the negative log likelihood of the correct class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()  # Combines sigmoid and BCE\n",
    "loss = bce_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5665)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers - What, Why, How, Types and Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Optimizers?\n",
    "Algorithms that adjust the network's parameters to minimize the loss function.\n",
    "\n",
    "### Why use Optimizers?\n",
    "They implement different strategies for updating weights, which can lead to faster convergence or better generalization.\n",
    "\n",
    "### How do Optimizers work?\n",
    "By computing gradients of the loss with respect to the parameters and updating them accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Types:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Stochastic Gradient Descent (SGD)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: The most basic form of gradient descent, updating parameters based on the gradient of the current mini-batch.\n",
    "\n",
    "**Why**: \n",
    "- Simple and memory-efficient\n",
    "- Can escape shallow local minima due to its stochastic nature\n",
    "\n",
    "**How**: \n",
    "- Compute the gradient of the loss with respect to each parameter\n",
    "- Update each parameter by subtracting the learning rate multiplied by its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Adam (Adaptive Moment Estimation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: An adaptive learning rate optimization algorithm that computes individual learning rates for different parameters.\n",
    "\n",
    "**Why**: \n",
    "- Combines the benefits of AdaGrad and RMSprop\n",
    "- Works well for problems with sparse gradients or noisy data\n",
    "\n",
    "**How**: \n",
    "- Maintains a moving average of the gradient and the squared gradient\n",
    "- Uses these to compute adaptive learning rates for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. RMSprop**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: An optimizer that adapts the learning rate for each parameter based on the recent gradient history.\n",
    "\n",
    "**Why**: \n",
    "- Addresses the diminishing learning rates problem of AdaGrad\n",
    "- Works well for non-stationary objectives\n",
    "\n",
    "**How**: \n",
    "- Maintains a moving average of squared gradients\n",
    "- Divides the learning rate by the square root of this average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - What, Why, How, Types and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Descent?\n",
    "An optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent.\n",
    "\n",
    "### Why use Gradient Descent?\n",
    "It provides a way to find the optimal parameters that minimize the loss function.\n",
    "\n",
    "### How does Gradient Descent work?\n",
    "By computing the gradient of the loss with respect to each parameter and updating the parameters in the opposite direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Batch Gradient Descent**\n",
    "\n",
    "**What**: Computes the gradient using the entire dataset.\n",
    "\n",
    "**Why**: \n",
    "- Provides a more accurate estimate of the gradient\n",
    "- Guaranteed to converge to the global minimum for convex error surfaces\n",
    "\n",
    "**How**: \n",
    "- Compute the gradient of the loss over the entire dataset\n",
    "- Update parameters once per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**What**: Computes the gradient using a single randomly selected sample.\n",
    "\n",
    "**Why**: \n",
    "- Faster than batch gradient descent for large datasets\n",
    "- Can escape local minima due to its noisy updates\n",
    "\n",
    "**How**: \n",
    "- Randomly select a single sample\n",
    "- Compute the gradient based on this sample\n",
    "- Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Mini-batch Gradient Descent**\n",
    "\n",
    "**What**: Computes the gradient using a small random subset of the data.\n",
    "\n",
    "**Why**: \n",
    "- Balances the efficiency of SGD with the stability of batch gradient descent\n",
    "- Allows for vectorized operations, which can be computationally efficient\n",
    "\n",
    "**How**: \n",
    "- Divide the dataset into mini-batches\n",
    "- Compute the gradient for each mini-batch\n",
    "- Update parameters after each mini-batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipostai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
