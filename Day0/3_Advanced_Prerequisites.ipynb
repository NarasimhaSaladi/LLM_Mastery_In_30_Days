{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED PREREQUISITES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL MACHINE TRANSLATION - ADVANCED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sequence-to-Sequence (Seq2Seq) Models:**\n",
    "Transform input sequences into output sequences, ideal for tasks like translation where both input and output are sequences of variable length.\n",
    "- **Encoders and Decoders:**\n",
    "Encoders compress input sequences into a fixed representation, while decoders generate output sequences from this representation, forming the core of Seq2Seq models.\n",
    "- **Bahdanau Attention Mechanism:**\n",
    "Allows the decoder to focus on different parts of the input sequence at each step, improving the model's ability to handle long sequences and complex relationships.\n",
    "- **Attention Neural Networks:**\n",
    "Incorporate attention mechanisms to dynamically weigh the importance of different input elements when generating each output element, enhancing translation quality and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE - MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a Seq2Seq model with attention. Here's a breakdown of the main components:\n",
    "\n",
    "**Encoder:** Processes the input sequence and returns the encoder outputs along with the final hidden and cell states.\n",
    "\n",
    "**Attention:** Implements the Bahdanau attention mechanism, calculating attention weights for each encoder output.\n",
    "\n",
    "**Decoder:** Generates the output sequence, using the attention mechanism to focus on relevant parts of the input.\n",
    "\n",
    "**Seq2SeqAttention:** Combines the encoder and decoder, implementing the forward pass of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is responsible for processing the input sequence and creating a representation that the decoder can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder uses an embedding layer to convert input tokens into dense vectors.\n",
    "- It then uses a multi-layer LSTM (Long Short-Term Memory) to process these embeddings.\n",
    "- The forward method returns:\n",
    "\n",
    "    ```outputs```: Contains the hidden state for each input token (useful for attention)\n",
    "\n",
    "    ```hidden```: The final hidden state of the LSTM\n",
    "    \n",
    "    ```cell```: The final cell state of the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAHDANANU ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bahdanau attention mechanism is a key innovation that allows the decoder to focus on different parts of the input sequence at each decoding step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The attention mechanism takes two inputs:\n",
    "\n",
    "    ```hidden```: The current hidden state of the decoder\n",
    "    \n",
    "    ```encoder_outputs```: All hidden states from the encoder\n",
    "\n",
    "\n",
    "- It calculates an \"energy\" score for each encoder output:\n",
    "\n",
    "    - First, it concatenates the decoder's hidden state with each encoder output.\n",
    "    - This concatenated vector is passed through a linear layer (self.attn) and a tanh activation.\n",
    "    - Another linear layer (self.v) reduces this to a single score.\n",
    "\n",
    "\n",
    "- The energy scores are converted to probabilities using softmax, creating the attention weights.\n",
    "\n",
    "- These weights determine how much focus to put on each part of the input sequence when generating the next output word.\n",
    "\n",
    "The key idea is that the model learns to pay attention to relevant parts of the input sequence, which is especially useful for long sequences or when certain input words are particularly important for the current output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder generates the output sequence one token at a time, using the attention mechanism to focus on relevant parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim + hidden_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 + embed_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden[-1], encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        \n",
    "        predicted = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0), embedded.squeeze(0)), dim=1))\n",
    "        \n",
    "        return predicted, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decoder first embeds the input token (which is either the true previous token during training, or the predicted previous token during inference).\n",
    "- It then uses the attention mechanism to compute attention weights over the encoder outputs.\n",
    "- These weights are used to create a weighted sum of the encoder outputs, called the context vector.\n",
    "- The embedded input is concatenated with the context vector and fed into the LSTM.\n",
    "- The output of the LSTM is concatenated with the context vector and the embedded input, then passed through a final linear layer to produce a probability distribution over the output vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEQ-SEQ WITH ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines the encoder and decoder into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(src.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles the full sequence-to-sequence process:\n",
    "\n",
    "- It first encodes the entire input sequence.\n",
    "- Then, it decodes one token at a time, using either the true previous token (teacher forcing) or the predicted previous token as input for the next step.\n",
    "- Teacher forcing is used randomly based on the teacher_forcing_ratio to balance between training stability and avoiding exposure bias.\n",
    "\n",
    "The Bahdanau attention mechanism is a crucial part of this model. It allows the decoder to focus on different parts of the input sequence at each decoding step, which is particularly useful for translation tasks where word order may differ between languages or where certain words may require context from various parts of the input sentence to translate correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE - TRAINING LOOP FOR TRANSLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Processing:**\n",
    "\n",
    "- Loads and tokenizes the Multi30k dataset (German to English translation)\n",
    "- Builds vocabularies for both languages\n",
    "- Creates data iterators for batching\n",
    "\n",
    "2. **Model Architecture:**\n",
    "\n",
    "- Implements an Encoder-Decoder architecture with attention\n",
    "- The Encoder uses an LSTM to process the input sequence\n",
    "- The Decoder uses another LSTM along with an attention mechanism to generate the output sequence\n",
    "\n",
    "3. **Training:**\n",
    "\n",
    "- Defines training and evaluation loops\n",
    "- Uses Adam optimizer and CrossEntropyLoss\n",
    "- Implements teacher forcing during training\n",
    "- Teacher forcing is a strategy for training recurrent neural networks that uses ground truth as input, instead of model output from a prior time step as an input.\n",
    "\n",
    "4. **Evaluation:**\n",
    "- **BLEU Score:** The BLEU score evaluates machine translations by comparing them to human translations, checking for word and phrase matches. It ranges from 0 to 1, with higher scores indicating better translation quality.\n",
    "- Provides functions to translate individual sentences\n",
    "- Calculates BLEU score on the test set to evaluate model performance\n",
    "\n",
    "#### **Main Execution:**\n",
    "\n",
    "- Creates the dataset and dataloader\n",
    "- Creates the model\n",
    "- Trains the model for a specified number of epochs\n",
    "- Saves the best model based on validation loss\n",
    "- Calculates and prints the BLEU score\n",
    "- Provides an example translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipostai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
